{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dirty_word(60%).ipynb",
      "provenance": [],
      "mount_file_id": "19K5Be2c4rRRl0wYbF_RtXDoaRaeJXXY6",
      "authorship_tag": "ABX9TyPkKXOzTJDgpFw/dScSb7DV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JL-plusUltra/DataScience/blob/main/Dirty_word(60_).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL0oP_53ubRM",
        "outputId": "69048c53-8b9f-4cb2-8dfc-1b55c6827cff"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd\n",
        "\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import math\n",
        "from pprint import pprint\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import string\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "# Import stopwords with nltk.\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "\n",
        "layers = keras.layers\n",
        "models = keras.models"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-88FlJ8Yu8DF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "89b125d2-6f20-4670-ad2c-f11350991070"
      },
      "source": [
        "data = pd.read_csv('/content/uw_new (1).csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>string</th>\n",
              "      <th>Label</th>\n",
              "      <th>Lab1</th>\n",
              "      <th>avarage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>*screams in 25 different languages*</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Families to sue over Legionnaires: More than 4...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>My emotions are a train wreck. My body is a tr...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Alton brown just did a livestream and he burne...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                             string  ...  Lab1  avarage\n",
              "0           0                *screams in 25 different languages*  ...     1        1\n",
              "1           1  Families to sue over Legionnaires: More than 4...  ...     3        2\n",
              "2           2  Pandemonium In Aba As Woman Delivers Baby With...  ...     2        1\n",
              "3           3  My emotions are a train wreck. My body is a tr...  ...     2        2\n",
              "4           4  Alton brown just did a livestream and he burne...  ...     0        0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K0HBYNDvJkx"
      },
      "source": [
        "#sia = SIA()\n",
        "\n",
        "#results = []\n",
        "\n",
        "#for line in data['string']:\n",
        " #   pol_score = sia.polarity_scores(line)\n",
        "  #  pol_score['headline'] = line\n",
        "   # results.append(pol_score)\n",
        "\n",
        "#pprint(results[:3], width=100)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMmym4ZWvlij"
      },
      "source": [
        "#data = pd.DataFrame.from_records(results)\n",
        "data.drop_duplicates(['string'])\n",
        "data['string']=data['string'].str.replace(r'https?:\\/\\/.*[\\r\\n]*','', case=False)\n",
        "data['string'] = data['string'].str.lower()\n",
        "#data['string'] = data['string'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "#data.head(5)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSfiwaGgPVyx"
      },
      "source": [
        "def cat(x):\n",
        "    if  x==0:\n",
        "      return 'No Danger'\n",
        "    if x==1:\n",
        "      return 'Risk'\n",
        "    if x==2:\n",
        "      return 'Risk'\n",
        "    if x==3:\n",
        "      return 'Danger'\n",
        "    if x == 4:\n",
        "      return 'Danger'\n",
        "data['classes'] = data['avarage'].apply(lambda x: cat(x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWc6SsDY891S"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l04tqIEQP-m0",
        "outputId": "95bd5567-6eca-4d75-9817-bfa0bb56ac36"
      },
      "source": [
        "data['classes'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Risk         824\n",
              "No Danger    814\n",
              "Danger       226\n",
              "Name: classes, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zT2fc6WOUuU"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YQrXvINMYzw",
        "outputId": "9496e0ff-b002-4d27-c933-250c3114771c"
      },
      "source": [
        "data.isna().sum()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0    0\n",
              "string        0\n",
              "Label         0\n",
              "Lab1          0\n",
              "avarage       0\n",
              "classes       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2om3Ac-8sO3"
      },
      "source": [
        "#def lemmatize_text(text):\n",
        " #   lemmatizer = WordNetLemmatizer()\n",
        "  #  return [lemmatizer.lemmatize(w) for w in text] "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyOj7H3AaJ2f"
      },
      "source": [
        "#lmtzr = WordNetLemmatizer()\n",
        "#data['lemmatize'] = data['headline'].apply(lambda lst:[lmtzr.lemmatize(word) for word in lst])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYJxuHIpYSLl"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlhXc6BAv-fk",
        "outputId": "b1ff38c1-a6f5-44ba-9f20-83ef7c9f8050"
      },
      "source": [
        "train_size = int(len(data) * .8)\n",
        "print (\"Train size: %d\" % train_size)\n",
        "print (\"Test size: %d\" % (len(data) - train_size))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 1491\n",
            "Test size: 373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdBjZnh_wCn-"
      },
      "source": [
        "def train_test_split(data, train_size):\n",
        "    train = data[:train_size]\n",
        "    test = data[train_size:]\n",
        "    return train, test"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQTUCVwfwkwX"
      },
      "source": [
        "train_cat, test_cat = train_test_split(data['classes'], train_size)\n",
        "train_text, test_text = train_test_split(data['string'], train_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m1AwxUnw2lP"
      },
      "source": [
        "max_words = 1000\n",
        "tokenize = keras.preprocessing.text.Tokenizer(num_words=max_words, char_level=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt-5CDdiw4id"
      },
      "source": [
        "tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data\n",
        "x_train = tokenize.texts_to_matrix(train_text)\n",
        "x_test = tokenize.texts_to_matrix(test_text)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nE8s_ruw-Zl"
      },
      "source": [
        "# Use sklearn utility to convert label strings to numbered index\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_cat)\n",
        "y_train = encoder.transform(train_cat)\n",
        "y_test = encoder.transform(test_cat)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm5TWwnOxDFC"
      },
      "source": [
        "# Converts the labels to a one-hot representation\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEGs9S08Tr1J"
      },
      "source": [
        "#hub_layer = hub.KerasLayer(embed, input_shape=[],dtype= tf.string, trainable=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BepCM3Q2xTMZ"
      },
      "source": [
        "batch_size = 5\n",
        "epochs = 10\n",
        "drop_ratio = .25"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz135zOfxUmV"
      },
      "source": [
        "# Build the model\n",
        "model = models.Sequential()\n",
        "#model.add(hub_layer)\n",
        "model.add(layers.Dense(4, input_shape=(max_words,)))\n",
        "model.add(layers.Dense(4))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Dropout(drop_ratio))\n",
        "model.add(layers.Dense(num_classes))\n",
        "model.add(layers.Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAcfEjv_xZIb",
        "outputId": "f1fb650d-529d-4ee8-aa52-40abfcead29b"
      },
      "source": [
        "# model.fit trains the model\n",
        "# The validation_split param tells Keras what % of our training data should be used in the validation set\n",
        "# You can see the validation loss decreasing slowly when you run this\n",
        "# Because val_loss is no longer decreasing we stop training to prevent overfitting\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "269/269 [==============================] - 1s 2ms/step - loss: 1.0136 - accuracy: 0.4497 - val_loss: 0.9398 - val_accuracy: 0.5667\n",
            "Epoch 2/10\n",
            "269/269 [==============================] - 0s 2ms/step - loss: 0.9219 - accuracy: 0.5160 - val_loss: 0.8906 - val_accuracy: 0.5800\n",
            "Epoch 3/10\n",
            "269/269 [==============================] - 0s 2ms/step - loss: 0.8524 - accuracy: 0.5533 - val_loss: 0.8614 - val_accuracy: 0.6000\n",
            "Epoch 4/10\n",
            "269/269 [==============================] - 0s 2ms/step - loss: 0.7724 - accuracy: 0.5913 - val_loss: 0.8433 - val_accuracy: 0.6000\n",
            "Epoch 5/10\n",
            "269/269 [==============================] - 0s 2ms/step - loss: 0.7063 - accuracy: 0.6212 - val_loss: 0.8314 - val_accuracy: 0.6200\n",
            "Epoch 6/10\n",
            "269/269 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.6480 - val_loss: 0.8306 - val_accuracy: 0.6000\n",
            "Epoch 7/10\n",
            "269/269 [==============================] - 0s 2ms/step - loss: 0.6196 - accuracy: 0.6823 - val_loss: 0.8543 - val_accuracy: 0.6200\n",
            "Epoch 8/10\n",
            "269/269 [==============================] - 0s 2ms/step - loss: 0.5778 - accuracy: 0.7353 - val_loss: 0.9062 - val_accuracy: 0.5933\n",
            "Epoch 9/10\n",
            "269/269 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7599 - val_loss: 0.9428 - val_accuracy: 0.6133\n",
            "Epoch 10/10\n",
            "269/269 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7584 - val_loss: 0.9863 - val_accuracy: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEcGZ976yzjg",
        "outputId": "46d2b3f8-cbe3-42b5-eb9c-5624dd0de6e8"
      },
      "source": [
        "# Evaluate the accuracy of our trained model\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 0s 2ms/step - loss: 0.9178 - accuracy: 0.5791\n",
            "Test loss: 0.9177872538566589\n",
            "Test accuracy: 0.5790884494781494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzwR28mU0TIf",
        "outputId": "8f85b5a4-ab84-4683-cbc8-1dfc44aa68fe"
      },
      "source": [
        "text_labels = encoder.classes_ \n",
        "\n",
        "for i in range(10):\n",
        "    prediction = model.predict(np.array([x_test[i]]))\n",
        "    predicted_label = text_labels[np.argmax(prediction)]\n",
        "    print(test_text.iloc[i][:50], \"...\")\n",
        "    print('Actual label:' + test_cat.iloc[i])\n",
        "    print(\"Predicted label: \" + predicted_label + \"\\n\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "however , it lacks grandeur and that epic quality  ...\n",
            "Actual label:No Danger\n",
            "Predicted label: No Danger\n",
            "\n",
            "invading iraq was a catastrophic mistake'.\n",
            "\n",
            "diplom ...\n",
            "Actual label:Risk\n",
            "Predicted label: Risk\n",
            "\n",
            "@babysweet420 i'm mad 420 in your name &amp; you d ...\n",
            "Actual label:Risk\n",
            "Predicted label: Risk\n",
            "\n",
            "philippines must protect internally displaced pers ...\n",
            "Actual label:No Danger\n",
            "Predicted label: Risk\n",
            "\n",
            "wild fires in california... must be global warming ...\n",
            "Actual label:Risk\n",
            "Predicted label: Risk\n",
            "\n",
            "just had a panic attack bc i don't have enough mon ...\n",
            "Actual label:Danger\n",
            "Predicted label: Risk\n",
            "\n",
            "my dogûªs just blown his kennel up ûò bloody yor ...\n",
            "Actual label:Risk\n",
            "Predicted label: Risk\n",
            "\n",
            "traffic accident n cabrillo hwy/magellan av mir (0 ...\n",
            "Actual label:Risk\n",
            "Predicted label: Risk\n",
            "\n",
            "do you feel like you are sinking in low self-image ...\n",
            "Actual label:Risk\n",
            "Predicted label: Risk\n",
            "\n",
            "unions say they are supportive of 'london' yet are ...\n",
            "Actual label:No Danger\n",
            "Predicted label: Risk\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48HeQsWF6b3D"
      },
      "source": [
        "tokenize.fit_on_texts([\"I wish my teacher choked to death\"])\n",
        "tokenize.fit_on_texts([\"I gave my teacher an apple\"])\n",
        "tokenize.fit_on_texts([\"I love my teddy\"])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grsgtBIG63Wb"
      },
      "source": [
        "a= tokenize.texts_to_matrix(np.array([\"I wish my teacher choked to death\"]))\n",
        "b= tokenize.texts_to_matrix(np.array([\"I gave my teacher an apple\"]))\n",
        "c= tokenize.texts_to_matrix(np.array([\"I love my teddy\"]))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eepdsU4h5pOb",
        "outputId": "4d0d4165-faf1-4e6a-86e6-adac0498f72a"
      },
      "source": [
        "pred = model.predict(a)\n",
        "d=text_labels[np.argmax(pred)]\n",
        "\n",
        "pred1 = model.predict(b)\n",
        "e=text_labels[np.argmax(pred1)]\n",
        "\n",
        "pred2 = model.predict(c)\n",
        "f=text_labels[np.argmax(pred2)]\n",
        "\n",
        "print(d,e,f)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Danger Risk Risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8i9UzLU5pMx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pclqd5XR1id6"
      },
      "source": [
        "y_softmax = model.predict(x_test)\n",
        "\n",
        "y_test_1d = []\n",
        "y_pred_1d = []\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    probs = y_test[i]\n",
        "    index_arr = np.nonzero(probs)\n",
        "    one_hot_index = index_arr[0].item(0)\n",
        "    y_test_1d.append(one_hot_index)\n",
        "\n",
        "for i in range(0, len(y_softmax)):\n",
        "    probs = y_softmax[i]\n",
        "    predicted_index = np.argmax(probs)\n",
        "    y_pred_1d.append(predicted_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGK_qqp51wSd"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=30)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45, fontsize=22)\n",
        "    plt.yticks(tick_marks, classes, fontsize=22)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label', fontsize=25)\n",
        "    plt.xlabel('Predicted label', fontsize=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85fGV7rx2BTE"
      },
      "source": [
        "cnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\n",
        "plt.figure(figsize=(10,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=text_labels, title=\"Confusion matrix\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}